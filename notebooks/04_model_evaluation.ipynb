{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a6cfa9b",
   "metadata": {},
   "source": [
    "## Validation Analysis for Selected Model (Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2eb929a3",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'src/models/best_random_forest_model.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Load the Random Forest model\u001b[39;00m\n\u001b[0;32m     19\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msrc/models/best_random_forest_model.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 20\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mjoblib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\joblib\\numpy_pickle.py:650\u001b[0m, in \u001b[0;36mload\u001b[1;34m(filename, mmap_mode)\u001b[0m\n\u001b[0;32m    648\u001b[0m         obj \u001b[38;5;241m=\u001b[39m _unpickle(fobj)\n\u001b[0;32m    649\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 650\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    651\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m _read_fileobject(f, filename, mmap_mode) \u001b[38;5;28;01mas\u001b[39;00m fobj:\n\u001b[0;32m    652\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fobj, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    653\u001b[0m                 \u001b[38;5;66;03m# if the returned file object is a string, this means we\u001b[39;00m\n\u001b[0;32m    654\u001b[0m                 \u001b[38;5;66;03m# try to load a pickle file generated with an version of\u001b[39;00m\n\u001b[0;32m    655\u001b[0m                 \u001b[38;5;66;03m# Joblib so we load it with joblib compatibility function.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'src/models/best_random_forest_model.pkl'"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load training and test data\n",
    "train_data_path = r'C:\\Users\\Denny\\insurance-risk-modeling\\data\\processed\\train_balanced_data.csv'\n",
    "train_df = pd.read_csv(train_data_path)\n",
    "X_train = train_df.drop(columns='Accident')\n",
    "y_train = train_df['Accident']\n",
    "\n",
    "test_data_path = r'C:\\Users\\Denny\\insurance-risk-modeling\\data\\processed\\test_balanced_data.csv'\n",
    "test_df = pd.read_csv(test_data_path)\n",
    "X_test = test_df.drop(columns='Accident')\n",
    "y_test = test_df['Accident']\n",
    "\n",
    "# Load the Random Forest model\n",
    "model_path = r'C:\\Users\\Denny\\insurance-risk-modeling\\src\\models\\random_forest_model.pkl'\n",
    "model = joblib.load(model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade6f6f4",
   "metadata": {},
   "source": [
    "### Performance on Train and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99082951",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize dictionary to store metrics for train and test data\n",
    "validation_metrics = {\n",
    "    'Dataset': [],\n",
    "    'Accuracy': [],\n",
    "    'Precision': [],\n",
    "    'Recall': [],\n",
    "    'F1 Score': [],\n",
    "    'AUC': []\n",
    "}\n",
    "\n",
    "# Evaluate on training data\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_train_proba = model.predict_proba(X_train)[:, 1]\n",
    "validation_metrics['Dataset'].append('Train')\n",
    "validation_metrics['Accuracy'].append(accuracy_score(y_train, y_train_pred))\n",
    "validation_metrics['Precision'].append(precision_score(y_train, y_train_pred))\n",
    "validation_metrics['Recall'].append(recall_score(y_train, y_train_pred))\n",
    "validation_metrics['F1 Score'].append(f1_score(y_train, y_train_pred))\n",
    "validation_metrics['AUC'].append(roc_auc_score(y_train, y_train_proba))\n",
    "\n",
    "# Evaluate on test data\n",
    "y_test_pred = model.predict(X_test)\n",
    "y_test_proba = model.predict_proba(X_test)[:, 1]\n",
    "validation_metrics['Dataset'].append('Test')\n",
    "validation_metrics['Accuracy'].append(accuracy_score(y_test, y_test_pred))\n",
    "validation_metrics['Precision'].append(precision_score(y_test, y_test_pred))\n",
    "validation_metrics['Recall'].append(recall_score(y_test, y_test_pred))\n",
    "validation_metrics['F1 Score'].append(f1_score(y_test, y_test_pred))\n",
    "validation_metrics['AUC'].append(roc_auc_score(y_test, y_test_proba))\n",
    "\n",
    "# Convert validation metrics to DataFrame for easier display\n",
    "validation_metrics_df = pd.DataFrame(validation_metrics)\n",
    "validation_metrics_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f681b4",
   "metadata": {},
   "source": [
    "### Train vs. Test Performance Comparison for Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f81367d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot comparison of train and test metrics for Random Forest\n",
    "metrics_to_plot = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'AUC']\n",
    "\n",
    "for metric in metrics_to_plot:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.barplot(data=validation_metrics_df, x='Dataset', y=metric)\n",
    "    plt.title(f'Train vs. Test {metric} for Random Forest')\n",
    "    plt.ylabel(metric)\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
